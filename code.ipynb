{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1IFvA-LfQi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "700fffdd-639e-402b-eab3-b22873b85d34"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q2ZLLg2hQpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8e0ebd0d-0853-42af-bdee-b035b7c65f13"
      },
      "source": [
        "#install if not available\n",
        "! pip install nltk\n",
        "\n",
        "print(\"***** Dependency library installed *****\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "***** Dependency library installed *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBvc-9Keg_hm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1150ecd-ffd1-4ec1-ab93-08452fb660a4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import concatenate, Activation, Conv1D, Conv2D, Softmax, Multiply, Add, Flatten, Dropout, Dense, Reshape, Lambda, BatchNormalization, Embedding, ZeroPadding1D, AveragePooling1D, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.backend import squeeze, sum, tile, clear_session\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import re\n",
        "import functools\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from functools import wraps\n",
        "\n",
        "\n",
        "featureSize = 100\n",
        "denseLayerNb = 5\n",
        "MAX_NUM_WORDS = 1000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "classNb = 2\n",
        "dataWords = 100\n",
        "batchSize = 128\n",
        "weight_path = '/content/drive/My Drive/NN_Project/data/weights/'\n",
        "train_dest = '/content/drive/My Drive/NN_Project/new_data/'\n",
        "dict_dir = '/content/drive/My Drive/NN_Project/data/glove.42B.300d.txt'\n",
        "\n",
        "print(\"***** Libraries imported and initilised values *****\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Libraries imported and initilised values *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8l6qLANhoEh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5b63b607-97a7-4d69-8502-3a5d6edcedef"
      },
      "source": [
        "#data import and process\n",
        "nltk.download('stopwords')\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(dict_dir)\n",
        "for line in f:\n",
        "    values = line.split(' ')\n",
        "    word = values[0] \n",
        "    coefs = np.asarray(values[1:], dtype='float32') \n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "trainLabel=[]\n",
        "trainData = []\n",
        "with open(train_dest + 'True.csv') as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "      field_names_list = row\n",
        "      trainLabel.append(\"1\")\n",
        "      if(len(field_names_list) > 0):\n",
        "        trainData.append(field_names_list[1].replace(\"...\", \" \"))\n",
        "with open(train_dest + 'Fake.csv') as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "      field_names_list = row\n",
        "      trainLabel.append(\"0\")\n",
        "      if(len(field_names_list) > 0):\n",
        "        trainData.append(field_names_list[1].replace(\"...\", \" \"))\n",
        "\n",
        "\n",
        "lines_without_stopwords=[] \n",
        "for line in trainData: \n",
        "  line = line.lower()\n",
        "  line_by_words = re.findall(r'(?:\\w+)', line, flags = re.UNICODE)\n",
        "  new_line=[]\n",
        "  for word in line_by_words:\n",
        "    if word not in stop:\n",
        "      new_line.append(word)\n",
        "  lines_without_stopwords.append(new_line)\n",
        "texts = lines_without_stopwords\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "labels = to_categorical(np.asarray(trainLabel))\n",
        "data, labels = shuffle(data, labels)\n",
        "\n",
        "EMBEDDING_DIM = embeddings_index.get('a').shape[0]\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(\"***** Data processed successfully *****\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "***** Data processed successfully *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kxokBZJiHpd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e288714-8596-4cf4-c35d-a22da422c046"
      },
      "source": [
        "#initilise utility functions\n",
        "\n",
        "def compose(*functions):\n",
        "    return functools.reduce(lambda f, g: lambda x: g(f(x)), functions, lambda x: x)\n",
        "\n",
        "def NormalisationAndActivation():\n",
        "    return compose(\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'))\n",
        "\n",
        "def convertFeature(*args, **kwargs):\n",
        "    return translationConvolutionWrapper(*args, **kwargs)\n",
        "\n",
        "def convolutionWrapper(*args, **kwargs):\n",
        "    ncKwargs = {}\n",
        "    ncKwargs.update(kwargs)\n",
        "    return Conv1D(*args, **ncKwargs)\n",
        "\n",
        "def denseWrapper(*args, **kwargs):\n",
        "    ndKwargs = {}\n",
        "    ndKwargs.update(kwargs)\n",
        "    return Dense(*args, **ndKwargs)\n",
        "\n",
        "def translationConvolutionWrapper(*args, **kwargs):\n",
        "    cfKwargs = {'kernel_size' : 1}\n",
        "    cfKwargs['filters'] = featureSize\n",
        "    cfKwargs['strides'] = 1\n",
        "    cfKwargs['padding'] = 'valid'\n",
        "    cfKwargs.update(kwargs)\n",
        "    return convolutionWrapper(*args, **cfKwargs)\n",
        "\n",
        "def denseConvolutionWrapper(*args, **kwargs):\n",
        "    dcKwargs = {'kernel_size' : 3}\n",
        "    dcKwargs['filters'] = featureSize\n",
        "    dcKwargs['strides'] = 1\n",
        "    dcKwargs['padding'] = 'valid'\n",
        "    dcKwargs.update(kwargs)\n",
        "    return compose(ZeroPadding1D(padding=1),\n",
        "                   convolutionWrapper(*args, **dcKwargs))\n",
        "\n",
        "def denseAvgPool(*args, **kwargs):\n",
        "    dapKwargs = {'pool_size' : 3}\n",
        "    dapKwargs['strides'] = 2\n",
        "    dapKwargs['padding'] = 'valid'\n",
        "    dapKwargs.update(kwargs)\n",
        "    return compose(ZeroPadding1D(padding=1),\n",
        "                   AveragePooling1D(*args, **dapKwargs))\n",
        "\n",
        "def groupConvolution(*args, **kwargs):\n",
        "    gcKwargs = {'kernel_size' : (1, 1)}\n",
        "    gcKwargs['filters'] = featureSize\n",
        "    gcKwargs['strides'] = 1\n",
        "    gcKwargs['padding'] = 'valid'\n",
        "    gcKwargs.update(kwargs)\n",
        "    return Conv2D(*args, **gcKwargs)\n",
        "\n",
        "def attention_function_1(*args, **kwargs):\n",
        "    af1Kwargs = {'units' : 64}\n",
        "    af1Kwargs.update(kwargs)\n",
        "    return compose(denseWrapper(*args, **af1Kwargs),\n",
        "                   Activation('relu'))\n",
        "\n",
        "def attention_function_2(*args, **kwargs):\n",
        "    af2Kwargs = {'units' : 32}\n",
        "    af2Kwargs.update(kwargs)\n",
        "    return compose(denseWrapper(*args, **af2Kwargs),\n",
        "                   Activation('relu'))\n",
        "\n",
        "def attention_function_3(*args, **kwargs):\n",
        "    af3Kwargs = {'units' : 4 + denseLayerNb - 1}\n",
        "    af3Kwargs.update(kwargs)\n",
        "    return compose(denseWrapper(*args, **af3Kwargs),\n",
        "                   Activation('relu'))\n",
        "\n",
        "def final_dropout_1(*args, **kwargs):\n",
        "    fc1Kwargs = {'units' : 64}\n",
        "    fc1Kwargs.update(kwargs)\n",
        "    return compose(Dropout(rate=0.5),\n",
        "                   denseWrapper(*args, **fc1Kwargs),\n",
        "                   Activation('relu'))\n",
        "\n",
        "def final_dropout_2(*args, **kwargs):\n",
        "    fc2Kwargs = {'units' : classNb}\n",
        "    fc2Kwargs.update(kwargs)\n",
        "    return compose(Dropout(rate=0.5),\n",
        "                   Dense(*args, **fc2Kwargs),\n",
        "                   Activation('relu'))\n",
        "\n",
        "\n",
        "def slice(x, h1, h2, w1, w2):\n",
        "    return x[:, h1 : h2, w1 : w2, :]\n",
        "\n",
        "def sliceChannel(x, c1, c2):\n",
        "    return x[:, :, c1 : c2]\n",
        "\n",
        "def sliceWords(x, w1, w2):\n",
        "    return x[:, w1 : w2, :]\n",
        "\n",
        "def attentionGroup(inputs, wordsLength):\n",
        "    outputs = inputs\n",
        "    for _ in range(2):\n",
        "        outputs = Reshape((outputs.shape[1], 4 + denseLayerNb - 1, featureSize))(outputs)\n",
        "        L = [Lambda(slice, arguments={'h1':0, 'h2':wordsLength, 'w1':i, 'w2':i + 1})(outputs) for i in range(4 + denseLayerNb - 1)]\n",
        "        L = [groupConvolution()(e) for e in L]\n",
        "        L = [Lambda(lambda z : squeeze(z, - 2))(e) for e in L]\n",
        "        outputs = concatenate(L)\n",
        "        outputs = NormalisationAndActivation()(outputs)\n",
        "    return outputs\n",
        "\n",
        "def sliceAndConcantinate(inputs, wordsLength):\n",
        "    L = [Lambda(sliceWords, arguments={'w1': i, 'w2': i + 1})(inputs) for i in range(wordsLength)]\n",
        "    L = [attention_function_1()(e) for e in L]\n",
        "    L = [attention_function_2()(e) for e in L]\n",
        "    L = [attention_function_3()(e) for e in L]\n",
        "    L = [Reshape((1, 4 + denseLayerNb - 1))(e) for e in L]\n",
        "    outputs = concatenate(L, axis=-2)\n",
        "    return outputs\n",
        "\n",
        "def generateAttentions(inputs):\n",
        "    outputsList = [Lambda(sliceChannel, arguments={'c1': i, 'c2': i + 1})(inputs) for i in range(4 + denseLayerNb - 1)]\n",
        "    outputsList = [Lambda(tile, arguments={'n' : (1, 1, featureSize)})(e) for e in outputsList]\n",
        "    return outputsList\n",
        "\n",
        "def linearWithDropout(inputs, classNb):\n",
        "    outputs = Flatten()(inputs)\n",
        "    outputs = compose(final_dropout_1(),\n",
        "                      final_dropout_2(units=classNb))(outputs)\n",
        "    return outputs\n",
        "\n",
        "def preProcess(inputs):\n",
        "    outputs = compose(Activation('relu'),\n",
        "                   Softmax())(inputs)\n",
        "    return outputs\n",
        "\n",
        "print(\"***** utility functions library initilised *****\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** utility functions library initilised *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODb_h18vi1wf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb91e776-6588-4fff-e23c-f49a9fdc8dcd"
      },
      "source": [
        "#Model definition \n",
        "\n",
        "def gen_trans_layer(inputs):\n",
        "    outputs1 = convertFeature()(inputs)\n",
        "    outputs = NormalisationAndActivation()(outputs1)\n",
        "    outputs2 = translationConvolutionWrapper()(outputs)\n",
        "    outputs = NormalisationAndActivation()(outputs2)\n",
        "    outputs3 = translationConvolutionWrapper()(outputs)\n",
        "    outputs = NormalisationAndActivation()(outputs3)\n",
        "    outputs4 = denseConvolutionWrapper()(outputs)\n",
        "    outputs = concatenate([outputs4, outputs1, outputs2, outputs3])\n",
        "    outputs = NormalisationAndActivation()(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def gen_dense_layer(inputs):\n",
        "    outputs = inputs\n",
        "    for _ in range(denseLayerNb - 1):\n",
        "        outputs1 = denseAvgPool()(outputs)\n",
        "        outputs2 = denseConvolutionWrapper()(outputs1)\n",
        "        outputs = concatenate([outputs2, outputs1])\n",
        "        outputs = NormalisationAndActivation()(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def gen_attention_layer(outputs, finalWordsLength):\n",
        "    outputs = attentionGroup(outputs, finalWordsLength)\n",
        "    xList = [Lambda(sliceChannel, arguments={'c1': i * featureSize, 'c2': (i + 1) * featureSize})(outputs) for i in range(4 + denseLayerNb - 1)]\n",
        "    outputsList = [Lambda(sum, arguments={'axis':-1, 'keepdims':True})(e) for e in xList]\n",
        "    outputs = concatenate(outputsList)\n",
        "    outputs = sliceAndConcantinate(outputs, finalWordsLength)\n",
        "    outputs = preProcess(outputs)\n",
        "    aList = generateAttentions(outputs)\n",
        "    reweightedList = [Multiply()([x, a]) for x, a in zip(xList, aList)]\n",
        "    outputs = Add()(reweightedList)\n",
        "    return outputs\n",
        "\n",
        "def gen_classification_layer(outputs):\n",
        "    outputs = linearWithDropout(outputs, classNb)\n",
        "    outputs = preProcess(outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def DenseNetFeatureAttentionModel(inputs, ebdMatrix, dataWords, classNb):\n",
        "    finalWordsLength = dataWords\n",
        "    layersNb = denseLayerNb - 1\n",
        "    pad = 1\n",
        "    winSize = 3\n",
        "    stride = 2\n",
        "    for _ in range(layersNb):\n",
        "        finalWordsLength = (finalWordsLength + pad * 2 - winSize) // stride + 1\n",
        "\n",
        "    outputs = Embedding(input_length=dataWords, input_dim=np.shape(ebdMatrix)[0], output_dim=np.shape(ebdMatrix)[1], mask_zero = False, trainable= False)(inputs)\n",
        "    outputs = gen_trans_layer(outputs)\n",
        "    outputs = gen_dense_layer(outputs)\n",
        "    outputs = gen_attention_layer(outputs, finalWordsLength)\n",
        "    outputs = gen_classification_layer(outputs)\n",
        "\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "print(\"***** model initilised *****\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** model initilised *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdyGB_G7fSiO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a61d5a7-102a-4f82-9dfd-10b29c0466cd"
      },
      "source": [
        "#training\n",
        "clear_session()\n",
        "data, labels = shuffle(data, labels)\n",
        "\n",
        "inputs = Input(shape=(dataWords, ))\n",
        "\n",
        "model = DenseNetFeatureAttentionModel(inputs, embedding_matrix, dataWords, classNb)\n",
        "\n",
        "model.summary()\n",
        "opt = Nadam(lr=0.00001, schedule_decay=0.0001)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])\n",
        "file_path = weight_path + \"100ws-adam-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
        "checkpoints = ModelCheckpoint(file_path,monitor='val_acc',verbose=1,save_best_only=True,mode='max')\n",
        "callback_list = [checkpoints]\n",
        "history = model.fit(data, labels, validation_split=0.4, epochs=50, batch_size=batchSize, callbacks=callback_list, verbose=1, shuffle=True)\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('news_accuracy_adam_100ws.png')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('news_loss_adam_100ws.png')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 100, 300)     300300      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 100, 100)     30100       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 100, 100)     400         conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 100, 100)     0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 100, 100)     10100       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 100, 100)     400         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 100, 100)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 100, 100)     10100       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 100, 100)     400         conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 100, 100)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d (ZeroPadding1D)  (None, 102, 100)     0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 100, 100)     30100       zero_padding1d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 100, 400)     0           conv1d_3[0][0]                   \n",
            "                                                                 conv1d[0][0]                     \n",
            "                                                                 conv1d_1[0][0]                   \n",
            "                                                                 conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 100, 400)     1600        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 100, 400)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_1 (ZeroPadding1D (None, 102, 400)     0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling1d (AveragePooli (None, 50, 400)      0           zero_padding1d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_2 (ZeroPadding1D (None, 52, 400)      0           average_pooling1d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 50, 100)      120100      zero_padding1d_2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 50, 500)      0           conv1d_4[0][0]                   \n",
            "                                                                 average_pooling1d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 50, 500)      2000        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 50, 500)      0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_3 (ZeroPadding1D (None, 52, 500)      0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling1d_1 (AveragePoo (None, 25, 500)      0           zero_padding1d_3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_4 (ZeroPadding1D (None, 27, 500)      0           average_pooling1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 25, 100)      150100      zero_padding1d_4[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 25, 600)      0           conv1d_5[0][0]                   \n",
            "                                                                 average_pooling1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 25, 600)      2400        concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 25, 600)      0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_5 (ZeroPadding1D (None, 27, 600)      0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling1d_2 (AveragePoo (None, 13, 600)      0           zero_padding1d_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_6 (ZeroPadding1D (None, 15, 600)      0           average_pooling1d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 13, 100)      180100      zero_padding1d_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 13, 700)      0           conv1d_6[0][0]                   \n",
            "                                                                 average_pooling1d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 13, 700)      2800        concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 13, 700)      0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_7 (ZeroPadding1D (None, 15, 700)      0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling1d_3 (AveragePoo (None, 7, 700)       0           zero_padding1d_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_8 (ZeroPadding1D (None, 9, 700)       0           average_pooling1d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 7, 100)       210100      zero_padding1d_8[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 7, 800)       0           conv1d_7[0][0]                   \n",
            "                                                                 average_pooling1d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 7, 800)       3200        concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 7, 800)       0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 7, 8, 100)    0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 7, 1, 100)    0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 7, 1, 100)    0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 7, 1, 100)    0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 7, 1, 100)    0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 7, 1, 100)    0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 7, 1, 100)    0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 7, 1, 100)    0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 7, 1, 100)    0           reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 7, 1, 100)    10100       lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 7, 1, 100)    10100       lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 7, 1, 100)    10100       lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 7, 1, 100)    10100       lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 7, 1, 100)    10100       lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 7, 1, 100)    10100       lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 7, 1, 100)    10100       lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 7, 1, 100)    10100       lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 7, 100)       0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 7, 100)       0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 7, 100)       0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 7, 100)       0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_12 (Lambda)              (None, 7, 100)       0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, 7, 100)       0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (None, 7, 100)       0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (None, 7, 100)       0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 7, 800)       0           lambda_8[0][0]                   \n",
            "                                                                 lambda_9[0][0]                   \n",
            "                                                                 lambda_10[0][0]                  \n",
            "                                                                 lambda_11[0][0]                  \n",
            "                                                                 lambda_12[0][0]                  \n",
            "                                                                 lambda_13[0][0]                  \n",
            "                                                                 lambda_14[0][0]                  \n",
            "                                                                 lambda_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 7, 800)       3200        concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 7, 800)       0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 7, 8, 100)    0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_16 (Lambda)              (None, 7, 1, 100)    0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (None, 7, 1, 100)    0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_18 (Lambda)              (None, 7, 1, 100)    0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_19 (Lambda)              (None, 7, 1, 100)    0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_20 (Lambda)              (None, 7, 1, 100)    0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_21 (Lambda)              (None, 7, 1, 100)    0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_22 (Lambda)              (None, 7, 1, 100)    0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_23 (Lambda)              (None, 7, 1, 100)    0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 7, 1, 100)    10100       lambda_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 7, 1, 100)    10100       lambda_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 7, 1, 100)    10100       lambda_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 7, 1, 100)    10100       lambda_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 7, 1, 100)    10100       lambda_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 7, 1, 100)    10100       lambda_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 7, 1, 100)    10100       lambda_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 7, 1, 100)    10100       lambda_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_24 (Lambda)              (None, 7, 100)       0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_25 (Lambda)              (None, 7, 100)       0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_26 (Lambda)              (None, 7, 100)       0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_27 (Lambda)              (None, 7, 100)       0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_28 (Lambda)              (None, 7, 100)       0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_29 (Lambda)              (None, 7, 100)       0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_30 (Lambda)              (None, 7, 100)       0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_31 (Lambda)              (None, 7, 100)       0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 7, 800)       0           lambda_24[0][0]                  \n",
            "                                                                 lambda_25[0][0]                  \n",
            "                                                                 lambda_26[0][0]                  \n",
            "                                                                 lambda_27[0][0]                  \n",
            "                                                                 lambda_28[0][0]                  \n",
            "                                                                 lambda_29[0][0]                  \n",
            "                                                                 lambda_30[0][0]                  \n",
            "                                                                 lambda_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 7, 800)       3200        concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 7, 800)       0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_32 (Lambda)              (None, 7, 100)       0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_33 (Lambda)              (None, 7, 100)       0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_34 (Lambda)              (None, 7, 100)       0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_35 (Lambda)              (None, 7, 100)       0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_36 (Lambda)              (None, 7, 100)       0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_37 (Lambda)              (None, 7, 100)       0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_38 (Lambda)              (None, 7, 100)       0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_39 (Lambda)              (None, 7, 100)       0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_40 (Lambda)              (None, 7, 1)         0           lambda_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_41 (Lambda)              (None, 7, 1)         0           lambda_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_42 (Lambda)              (None, 7, 1)         0           lambda_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_43 (Lambda)              (None, 7, 1)         0           lambda_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_44 (Lambda)              (None, 7, 1)         0           lambda_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_45 (Lambda)              (None, 7, 1)         0           lambda_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_46 (Lambda)              (None, 7, 1)         0           lambda_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_47 (Lambda)              (None, 7, 1)         0           lambda_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 7, 8)         0           lambda_40[0][0]                  \n",
            "                                                                 lambda_41[0][0]                  \n",
            "                                                                 lambda_42[0][0]                  \n",
            "                                                                 lambda_43[0][0]                  \n",
            "                                                                 lambda_44[0][0]                  \n",
            "                                                                 lambda_45[0][0]                  \n",
            "                                                                 lambda_46[0][0]                  \n",
            "                                                                 lambda_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_48 (Lambda)              (None, 1, 8)         0           concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_49 (Lambda)              (None, 1, 8)         0           concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_50 (Lambda)              (None, 1, 8)         0           concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_51 (Lambda)              (None, 1, 8)         0           concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_52 (Lambda)              (None, 1, 8)         0           concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_53 (Lambda)              (None, 1, 8)         0           concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_54 (Lambda)              (None, 1, 8)         0           concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1, 64)        576         lambda_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1, 64)        576         lambda_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1, 64)        576         lambda_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1, 64)        576         lambda_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1, 64)        576         lambda_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1, 64)        576         lambda_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1, 64)        576         lambda_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 1, 64)        0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 1, 64)        0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 1, 64)        0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 1, 64)        0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 1, 64)        0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 1, 64)        0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 1, 64)        0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1, 32)        2080        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 1, 32)        2080        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 1, 32)        2080        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 1, 32)        2080        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1, 32)        2080        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 1, 32)        2080        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 1, 32)        2080        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 1, 32)        0           dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 1, 32)        0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 1, 32)        0           dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 1, 32)        0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 1, 32)        0           dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 1, 32)        0           dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 1, 32)        0           dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 1, 8)         264         activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 1, 8)         264         activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 1, 8)         264         activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 1, 8)         264         activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 1, 8)         264         activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 1, 8)         264         activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 1, 8)         264         activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 1, 8)         0           dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 1, 8)         0           dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 1, 8)         0           dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 1, 8)         0           dense_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 1, 8)         0           dense_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 1, 8)         0           dense_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 1, 8)         0           dense_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 1, 8)         0           activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 1, 8)         0           activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_4 (Reshape)             (None, 1, 8)         0           activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 1, 8)         0           activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_6 (Reshape)             (None, 1, 8)         0           activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_7 (Reshape)             (None, 1, 8)         0           activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "reshape_8 (Reshape)             (None, 1, 8)         0           activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 7, 8)         0           reshape_2[0][0]                  \n",
            "                                                                 reshape_3[0][0]                  \n",
            "                                                                 reshape_4[0][0]                  \n",
            "                                                                 reshape_5[0][0]                  \n",
            "                                                                 reshape_6[0][0]                  \n",
            "                                                                 reshape_7[0][0]                  \n",
            "                                                                 reshape_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 7, 8)         0           concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Softmax)               (None, 7, 8)         0           activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lambda_55 (Lambda)              (None, 7, 1)         0           softmax[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_56 (Lambda)              (None, 7, 1)         0           softmax[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_57 (Lambda)              (None, 7, 1)         0           softmax[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_58 (Lambda)              (None, 7, 1)         0           softmax[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_59 (Lambda)              (None, 7, 1)         0           softmax[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_60 (Lambda)              (None, 7, 1)         0           softmax[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_61 (Lambda)              (None, 7, 1)         0           softmax[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_62 (Lambda)              (None, 7, 1)         0           softmax[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_63 (Lambda)              (None, 7, 100)       0           lambda_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_64 (Lambda)              (None, 7, 100)       0           lambda_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_65 (Lambda)              (None, 7, 100)       0           lambda_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_66 (Lambda)              (None, 7, 100)       0           lambda_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_67 (Lambda)              (None, 7, 100)       0           lambda_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_68 (Lambda)              (None, 7, 100)       0           lambda_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_69 (Lambda)              (None, 7, 100)       0           lambda_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_70 (Lambda)              (None, 7, 100)       0           lambda_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 7, 100)       0           lambda_32[0][0]                  \n",
            "                                                                 lambda_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 7, 100)       0           lambda_33[0][0]                  \n",
            "                                                                 lambda_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 7, 100)       0           lambda_34[0][0]                  \n",
            "                                                                 lambda_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_3 (Multiply)           (None, 7, 100)       0           lambda_35[0][0]                  \n",
            "                                                                 lambda_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_4 (Multiply)           (None, 7, 100)       0           lambda_36[0][0]                  \n",
            "                                                                 lambda_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_5 (Multiply)           (None, 7, 100)       0           lambda_37[0][0]                  \n",
            "                                                                 lambda_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_6 (Multiply)           (None, 7, 100)       0           lambda_38[0][0]                  \n",
            "                                                                 lambda_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "multiply_7 (Multiply)           (None, 7, 100)       0           lambda_39[0][0]                  \n",
            "                                                                 lambda_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 7, 100)       0           multiply[0][0]                   \n",
            "                                                                 multiply_1[0][0]                 \n",
            "                                                                 multiply_2[0][0]                 \n",
            "                                                                 multiply_3[0][0]                 \n",
            "                                                                 multiply_4[0][0]                 \n",
            "                                                                 multiply_5[0][0]                 \n",
            "                                                                 multiply_6[0][0]                 \n",
            "                                                                 multiply_7[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 700)          0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 700)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 64)           44864       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 64)           0           dense_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 64)           0           activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 2)            130         dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 2)            0           dense_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 2)            0           activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "softmax_1 (Softmax)             (None, 2)            0           activation_34[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,287,734\n",
            "Trainable params: 977,634\n",
            "Non-trainable params: 310,100\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "211/211 [==============================] - ETA: 0s - loss: 0.8383 - acc: 0.5191\n",
            "Epoch 00001: val_acc improved from -inf to 0.52511, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-01-0.5251.hdf5\n",
            "211/211 [==============================] - 15s 72ms/step - loss: 0.8383 - acc: 0.5191 - val_loss: 0.6931 - val_acc: 0.5251\n",
            "Epoch 2/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.7446 - acc: 0.5286\n",
            "Epoch 00002: val_acc did not improve from 0.52511\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.7445 - acc: 0.5289 - val_loss: 0.6956 - val_acc: 0.5251\n",
            "Epoch 3/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.7134 - acc: 0.5358\n",
            "Epoch 00003: val_acc improved from 0.52511 to 0.52739, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-03-0.5274.hdf5\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.7135 - acc: 0.5357 - val_loss: 0.6928 - val_acc: 0.5274\n",
            "Epoch 4/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.7022 - acc: 0.5358\n",
            "Epoch 00004: val_acc improved from 0.52739 to 0.52756, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-04-0.5276.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.7023 - acc: 0.5362 - val_loss: 0.6924 - val_acc: 0.5276\n",
            "Epoch 5/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.5404\n",
            "Epoch 00005: val_acc did not improve from 0.52756\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.6924 - acc: 0.5406 - val_loss: 0.6925 - val_acc: 0.5264\n",
            "Epoch 6/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.6897 - acc: 0.5439\n",
            "Epoch 00006: val_acc did not improve from 0.52756\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.6897 - acc: 0.5437 - val_loss: 0.6920 - val_acc: 0.5265\n",
            "Epoch 7/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.6871 - acc: 0.5441\n",
            "Epoch 00007: val_acc did not improve from 0.52756\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.6871 - acc: 0.5442 - val_loss: 0.6909 - val_acc: 0.5267\n",
            "Epoch 8/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.6792 - acc: 0.5480\n",
            "Epoch 00008: val_acc did not improve from 0.52756\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.6793 - acc: 0.5480 - val_loss: 0.6874 - val_acc: 0.5276\n",
            "Epoch 9/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.6727 - acc: 0.5539\n",
            "Epoch 00009: val_acc improved from 0.52756 to 0.53035, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-09-0.5303.hdf5\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.6727 - acc: 0.5538 - val_loss: 0.6787 - val_acc: 0.5303\n",
            "Epoch 10/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.6567 - acc: 0.5709\n",
            "Epoch 00010: val_acc improved from 0.53035 to 0.54220, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-10-0.5422.hdf5\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.6567 - acc: 0.5712 - val_loss: 0.6533 - val_acc: 0.5422\n",
            "Epoch 11/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.6369 - acc: 0.5903\n",
            "Epoch 00011: val_acc improved from 0.54220 to 0.57567, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-11-0.5757.hdf5\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.6367 - acc: 0.5904 - val_loss: 0.6145 - val_acc: 0.5757\n",
            "Epoch 12/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.6059 - acc: 0.6297\n",
            "Epoch 00012: val_acc improved from 0.57567 to 0.66793, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-12-0.6679.hdf5\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.6059 - acc: 0.6298 - val_loss: 0.5567 - val_acc: 0.6679\n",
            "Epoch 13/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.5658 - acc: 0.6711\n",
            "Epoch 00013: val_acc improved from 0.66793 to 0.78040, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-13-0.7804.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.5663 - acc: 0.6711 - val_loss: 0.4850 - val_acc: 0.7804\n",
            "Epoch 14/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.5180 - acc: 0.7202\n",
            "Epoch 00014: val_acc improved from 0.78040 to 0.83318, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-14-0.8332.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.5179 - acc: 0.7203 - val_loss: 0.4171 - val_acc: 0.8332\n",
            "Epoch 15/50\n",
            "211/211 [==============================] - ETA: 0s - loss: 0.4730 - acc: 0.7600\n",
            "Epoch 00015: val_acc improved from 0.83318 to 0.86409, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-15-0.8641.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.4730 - acc: 0.7600 - val_loss: 0.3627 - val_acc: 0.8641\n",
            "Epoch 16/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.4247 - acc: 0.7987\n",
            "Epoch 00016: val_acc did not improve from 0.86409\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.4248 - acc: 0.7988 - val_loss: 0.3401 - val_acc: 0.8570\n",
            "Epoch 17/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8280\n",
            "Epoch 00017: val_acc improved from 0.86409 to 0.89137, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-17-0.8914.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.3768 - acc: 0.8279 - val_loss: 0.2713 - val_acc: 0.8914\n",
            "Epoch 18/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8499\n",
            "Epoch 00018: val_acc improved from 0.89137 to 0.89933, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-18-0.8993.hdf5\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.3382 - acc: 0.8498 - val_loss: 0.2526 - val_acc: 0.8993\n",
            "Epoch 19/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.3048 - acc: 0.8701\n",
            "Epoch 00019: val_acc improved from 0.89933 to 0.91147, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-19-0.9115.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.3048 - acc: 0.8701 - val_loss: 0.2269 - val_acc: 0.9115\n",
            "Epoch 20/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.2798 - acc: 0.8829\n",
            "Epoch 00020: val_acc improved from 0.91147 to 0.91954, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-20-0.9195.hdf5\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.2797 - acc: 0.8830 - val_loss: 0.2073 - val_acc: 0.9195\n",
            "Epoch 21/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.2488 - acc: 0.8956\n",
            "Epoch 00021: val_acc improved from 0.91954 to 0.92996, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-21-0.9300.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.2491 - acc: 0.8956 - val_loss: 0.1846 - val_acc: 0.9300\n",
            "Epoch 22/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9051\n",
            "Epoch 00022: val_acc improved from 0.92996 to 0.93229, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-22-0.9323.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.2349 - acc: 0.9052 - val_loss: 0.1748 - val_acc: 0.9323\n",
            "Epoch 23/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.2098 - acc: 0.9157\n",
            "Epoch 00023: val_acc improved from 0.93229 to 0.93374, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-23-0.9337.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.2099 - acc: 0.9156 - val_loss: 0.1730 - val_acc: 0.9337\n",
            "Epoch 24/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.1972 - acc: 0.9217\n",
            "Epoch 00024: val_acc improved from 0.93374 to 0.93864, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-24-0.9386.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.1972 - acc: 0.9217 - val_loss: 0.1591 - val_acc: 0.9386\n",
            "Epoch 25/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.9273\n",
            "Epoch 00025: val_acc improved from 0.93864 to 0.94243, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-25-0.9424.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.1849 - acc: 0.9272 - val_loss: 0.1491 - val_acc: 0.9424\n",
            "Epoch 26/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.1722 - acc: 0.9337\n",
            "Epoch 00026: val_acc improved from 0.94243 to 0.94566, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-26-0.9457.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.1722 - acc: 0.9337 - val_loss: 0.1417 - val_acc: 0.9457\n",
            "Epoch 27/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.1664 - acc: 0.9368\n",
            "Epoch 00027: val_acc improved from 0.94566 to 0.94794, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-27-0.9479.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.1665 - acc: 0.9368 - val_loss: 0.1361 - val_acc: 0.9479\n",
            "Epoch 28/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.9408\n",
            "Epoch 00028: val_acc improved from 0.94794 to 0.95006, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-28-0.9501.hdf5\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.1540 - acc: 0.9408 - val_loss: 0.1318 - val_acc: 0.9501\n",
            "Epoch 29/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9456\n",
            "Epoch 00029: val_acc improved from 0.95006 to 0.95239, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-29-0.9524.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.1446 - acc: 0.9457 - val_loss: 0.1260 - val_acc: 0.9524\n",
            "Epoch 30/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9484\n",
            "Epoch 00030: val_acc improved from 0.95239 to 0.95395, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-30-0.9540.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.1380 - acc: 0.9484 - val_loss: 0.1233 - val_acc: 0.9540\n",
            "Epoch 31/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9511\n",
            "Epoch 00031: val_acc improved from 0.95395 to 0.95490, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-31-0.9549.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.1303 - acc: 0.9511 - val_loss: 0.1209 - val_acc: 0.9549\n",
            "Epoch 32/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.1195 - acc: 0.9553\n",
            "Epoch 00032: val_acc improved from 0.95490 to 0.95668, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-32-0.9567.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.1196 - acc: 0.9552 - val_loss: 0.1192 - val_acc: 0.9567\n",
            "Epoch 33/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9581\n",
            "Epoch 00033: val_acc improved from 0.95668 to 0.95963, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-33-0.9596.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.1131 - acc: 0.9582 - val_loss: 0.1123 - val_acc: 0.9596\n",
            "Epoch 34/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.1093 - acc: 0.9595\n",
            "Epoch 00034: val_acc did not improve from 0.95963\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.1093 - acc: 0.9595 - val_loss: 0.1184 - val_acc: 0.9574\n",
            "Epoch 35/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0998 - acc: 0.9638\n",
            "Epoch 00035: val_acc improved from 0.95963 to 0.96058, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-35-0.9606.hdf5\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.0998 - acc: 0.9638 - val_loss: 0.1083 - val_acc: 0.9606\n",
            "Epoch 36/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0915 - acc: 0.9658\n",
            "Epoch 00036: val_acc improved from 0.96058 to 0.96281, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-36-0.9628.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.0919 - acc: 0.9658 - val_loss: 0.1071 - val_acc: 0.9628\n",
            "Epoch 37/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9659\n",
            "Epoch 00037: val_acc improved from 0.96281 to 0.96331, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-37-0.9633.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.0926 - acc: 0.9659 - val_loss: 0.1043 - val_acc: 0.9633\n",
            "Epoch 38/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9691\n",
            "Epoch 00038: val_acc improved from 0.96331 to 0.96442, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-38-0.9644.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.0863 - acc: 0.9692 - val_loss: 0.1024 - val_acc: 0.9644\n",
            "Epoch 39/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9699\n",
            "Epoch 00039: val_acc improved from 0.96442 to 0.96526, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-39-0.9653.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.0807 - acc: 0.9699 - val_loss: 0.1019 - val_acc: 0.9653\n",
            "Epoch 40/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9718\n",
            "Epoch 00040: val_acc improved from 0.96526 to 0.96587, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-40-0.9659.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.0780 - acc: 0.9717 - val_loss: 0.0991 - val_acc: 0.9659\n",
            "Epoch 41/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9741\n",
            "Epoch 00041: val_acc improved from 0.96587 to 0.96637, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-41-0.9664.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.0723 - acc: 0.9741 - val_loss: 0.0987 - val_acc: 0.9664\n",
            "Epoch 42/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9765\n",
            "Epoch 00042: val_acc improved from 0.96637 to 0.96732, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-42-0.9673.hdf5\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.0688 - acc: 0.9765 - val_loss: 0.0985 - val_acc: 0.9673\n",
            "Epoch 43/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9778\n",
            "Epoch 00043: val_acc improved from 0.96732 to 0.96782, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-43-0.9678.hdf5\n",
            "211/211 [==============================] - 14s 66ms/step - loss: 0.0633 - acc: 0.9777 - val_loss: 0.0998 - val_acc: 0.9678\n",
            "Epoch 44/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9804\n",
            "Epoch 00044: val_acc improved from 0.96782 to 0.96821, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-44-0.9682.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.0568 - acc: 0.9805 - val_loss: 0.0967 - val_acc: 0.9682\n",
            "Epoch 45/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9808\n",
            "Epoch 00045: val_acc improved from 0.96821 to 0.96932, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-45-0.9693.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.0562 - acc: 0.9808 - val_loss: 0.0961 - val_acc: 0.9693\n",
            "Epoch 46/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9821\n",
            "Epoch 00046: val_acc did not improve from 0.96932\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.0510 - acc: 0.9821 - val_loss: 0.0953 - val_acc: 0.9689\n",
            "Epoch 47/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9837\n",
            "Epoch 00047: val_acc did not improve from 0.96932\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.0491 - acc: 0.9837 - val_loss: 0.0963 - val_acc: 0.9689\n",
            "Epoch 48/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9844\n",
            "Epoch 00048: val_acc improved from 0.96932 to 0.97004, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-48-0.9700.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.0461 - acc: 0.9843 - val_loss: 0.0965 - val_acc: 0.9700\n",
            "Epoch 49/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9848\n",
            "Epoch 00049: val_acc improved from 0.97004 to 0.97060, saving model to /content/drive/My Drive/NN_Project/data/weights/100ws-adam-49-0.9706.hdf5\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.0442 - acc: 0.9848 - val_loss: 0.0967 - val_acc: 0.9706\n",
            "Epoch 50/50\n",
            "210/211 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9870\n",
            "Epoch 00050: val_acc did not improve from 0.97060\n",
            "211/211 [==============================] - 14s 65ms/step - loss: 0.0394 - acc: 0.9870 - val_loss: 0.0977 - val_acc: 0.9706\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}